{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "\n",
    "from transformers import ElectraModel, ElectraTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = os.listdir('./inference_data')\n",
    "file_path = []\n",
    "for file_name in file_list:\n",
    "    tmp_path = os.path.join('./inference_data', file_name)\n",
    "    file_path.append(tmp_path)\n",
    "file_path.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of ./inference_data/event_port.csv: 171\n"
     ]
    }
   ],
   "source": [
    "load_df = lambda x: pd.read_csv(x)\n",
    "dfs = [load_df(x) for x in file_path]\n",
    "infer_data = dfs[0]\n",
    "infer_data.dropna(inplace=True)\n",
    "# infer_data[pd.isna(infer_data).any(axis=1)]\n",
    "# infer_data.isna().nunique()\n",
    "print(f'length of {file_path[0]}: {len(infer_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Event    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, row in infer_data.iterrows():\n",
    "#     print(row.Event)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(df):\n",
    "#     df['Event'] = df['Event'].str.replace(\"．\", \".\", regex=False)\n",
    "#     df['Event'] = df['Event'].astype(str)\n",
    "#     # df['Event'] = df['Event'].str.replace(r'[^ㄱ-ㅣ가-힣0-9a-zA-Z.]+', \" \", regex=True)\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer_data = preprocess(infer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ElectraTokenizer.from_pretrained('./tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[CLS]', '[PAD]', '[UNK]', '[SEP]')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_token = tokenizer.cls_token\n",
    "sep_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "init_token, pad_token, unk_token, sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(171, 171)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_data_lst = infer_data.Event.apply(lambda x: tokenizer(x, max_length=256, padding='max_length', truncation=True))\n",
    "input_ids_lst = []\n",
    "input_masks_lst = []\n",
    "for e in infer_data_lst:\n",
    "    input_ids_lst.append(e['input_ids'])\n",
    "    input_masks_lst.append(e['attention_mask'])\n",
    "len(input_ids_lst), len(input_masks_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 7102, 4556, 23, 4501, 6338, 4366, 6224, 30, 7203, 4158, 35137, 14881, 17303, 3130, 36053, 16030, 21211, 4292, 6237, 35045, 3330, 3242, 9214, 35311, 12, 35156, 13, 6215, 24, 2351, 4292, 3027, 4112, 3826, 25, 4071, 4292, 3027, 4031, 8416, 4044, 16030, 20788, 36077, 3232, 2151, 4219, 12282, 4820, 3330, 35037, 2010, 15893, 24542, 4181, 4129, 2682, 4234, 3330, 3014, 4292, 3245, 4219, 2771, 2672, 4047, 16030, 6314, 6800, 4073, 9885, 26509, 6904, 4283, 7087, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \n",
      "\n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(input_ids_lst[0], '\\n\\n', input_masks_lst[0])\n",
    "print(input_ids_lst[0], '\\n\\n', input_masks_lst[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTPoSTagger(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 output_dim, \n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, text, mask):\n",
    "        #text = [batch size, sent len]\n",
    "        embedded = self.dropout(self.bert(text, mask)[0])\n",
    "        #embedded = [batch size, seq len, emb dim]\n",
    "        predictions = self.fc(self.dropout(embedded))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36223\n"
     ]
    }
   ],
   "source": [
    "bert = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(36223, 768)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIM = 22\n",
    "DROPOUT = 0.25\n",
    "model = BERTPoSTagger(bert, OUTPUT_DIM, DROPOUT)\n",
    "model.bert.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "NGPU = torch.cuda.device_count()\n",
    "if NGPU > 1:\n",
    "    model = torch.nn.DataParallel(model, device_ids=list(range(NGPU)))\n",
    "    # torch.multiprocessing.set_start_method('spawn', force=True)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"tut2-model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {'[PAD]': 0, '-': 1, 'AFW_B': 2, 'AFW_I': 3, 'ANM_B': 4, 'ANM_I': 5, 'CVL_B': 6, 'CVL_I': 7, 'DAT_B': 8, 'DAT_I': 9, 'LOC_B': 10, 'LOC_I': 11, 'MAT_B': 12, 'MAT_I': 13, 'NUM_B': 14, 'NUM_I': 15, 'TIM_B': 16, 'TIM_I': 17, 'TRM_B': 18, 'TRM_I': 19, 'WRK_B': 20, 'WRK_I': 21}\n",
    "id2label = {0: '[PAD]', 1: '-', 2: 'AFW_B', 3: 'AFW_I', 4: 'ANM_B', 5: 'ANM_I', 6: 'CVL_B', 7: 'CVL_I', 8: 'DAT_B', 9: 'DAT_I', 10: 'LOC_B', 11: 'LOC_I', 12: 'MAT_B', 13: 'MAT_I', 14: 'NUM_B', 15: 'NUM_I', 16: 'TIM_B', 17: 'TIM_I', 18: 'TRM_B', 19: 'TRM_I', 20: 'WRK_B', 21: 'WRK_I'}\n",
    "labels = ['[PAD]', '-', 'AFW_B', 'AFW_I', 'ANM_B', 'ANM_I', 'CVL_B', 'CVL_I', 'DAT_B', 'DAT_I', 'LOC_B', 'LOC_I', 'MAT_B', 'MAT_I', 'NUM_B', 'NUM_I', 'TIM_B', 'TIM_I', 'TRM_B', 'TRM_I', 'WRK_B', 'WRK_I']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    # infer_data.Event.apply(lambda x: tokenizer(x, max_length=256, padding='max_length', truncation=True))\n",
    "\n",
    "\n",
    "    infer_results = []\n",
    "    for input_ids, input_masks in zip(input_ids_lst, input_masks_lst): \n",
    "        # print(input_ids, input_masks)\n",
    "        input_tensor = torch.LongTensor(input_ids).unsqueeze(0).to(device)\n",
    "        mask_tensor = torch.LongTensor(input_masks).unsqueeze(0).to(device)\n",
    "        output = model(input_tensor, mask_tensor)\n",
    "        pred = torch.argmax(output, dim=-1).squeeze().detach().cpu().tolist()\n",
    "        infer_results.append(pred)\n",
    "        break\n",
    "\n",
    "    \n",
    "    # for idx, _ in enumerate(selected_sample):\n",
    "    #     item = selected_sample[idx]\n",
    "    #     input_ids_lst = list(map(lambda x:tokenizer.convert_tokens_to_ids(x), item[0]))\n",
    "    #     input_tensor = torch.LongTensor(input_ids_lst).unsqueeze(0).to(device)\n",
    "    #     mask_tensor = torch.LongTensor(item[1]).unsqueeze(0).to(device)\n",
    "    #     output = model(input_tensor, mask_tensor)\n",
    "    #     pred = torch.argmax(output, dim=-1).squeeze().detach().cpu().tolist()\n",
    "    #     sample_results.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD]'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2015', '##년', '3', '##월', '15', '##일', '20', ':', '35', '##경', '○○', '에', '선적', '##을', '위해', '인', '(', ')', '에서', '단', '##을', '쌓', '##은', '후', '5', '##단', '##을', '쌓', '##기', '위하', '##여', '을', '깔', '##고', '내려오', '##던', '가', '발이', '미끄러지', '##면', '##서', '몸', '##의', '심', '##을', '잃', '##고', '면', '##과', '사이', '공간', '##에', '추락', '##하여', '사망', '##한', '사례']\n",
      "57\n",
      "['DAT_B', 'DAT_I', 'DAT_I', 'DAT_I', 'DAT_I', 'DAT_I', 'TIM_B', 'TIM_I', 'TIM_I', 'TIM_I', 'LOC_B', 'LOC_I', 'LOC_I', 'LOC_I', '-', '-', '-', '-', '-', '-', '-', '-', 'LOC_B', 'LOC_I', 'LOC_I', '-', 'LOC_I', 'LOC_I', 'NUM_B', 'NUM_I', 'NUM_I', '-', '-', '-', 'NUM_B', 'NUM_I', 'NUM_I', '-', '-', '-', '-', '-', '-', '-', 'WRK_I', '-', '-', '-', '-', '-', 'CVL_B', 'CVL_I', 'ANM_B', '-', '-', '-']\n"
     ]
    }
   ],
   "source": [
    "decoded = tokenizer.decode(input_ids_lst[0], skip_special_tokens=True)\n",
    "subwords = tokenizer.encode(decoded)[1:-1]\n",
    "subwords = list(map(lambda x: tokenizer.convert_ids_to_tokens(x), subwords))\n",
    "# subwords = tokenizer.decode(subwords)\n",
    "print(subwords)\n",
    "\n",
    "\n",
    "lenn = len(subwords)\n",
    "print(lenn)\n",
    "\n",
    "\n",
    "toLabel = lambda x: id2label[x]\n",
    "tags = []\n",
    "for result in infer_results:\n",
    "    result = result[1:lenn]\n",
    "    for id in result:\n",
    "        tags.append(toLabel(id))\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LOC_B', 'LOC_I', 'LOC_I', 'LOC_I']\n",
      "○○ 에 선적을\n"
     ]
    }
   ],
   "source": [
    "# 날짜(DAT), 시간(TIME), 장소(LOC), 작업 정보(WRK)를 추출해서 txt 파일에 저장\n",
    "\n",
    "def extract(tagtype, tags):\n",
    "    # if tag does not exists then notify and pass\n",
    "    # if not '{tag}_I' then stop\n",
    "    tag_start = f'{tagtype}_B'\n",
    "    if tag_start in tags:\n",
    "        b_idx = tags.index(tag_start)\n",
    "        for i, tag in enumerate(tags[b_idx:]):\n",
    "            if i > 0 and tag != f'{tagtype}_I':\n",
    "                # print(tags[b_idx:b_idx+i])\n",
    "                tagged = subwords[b_idx:b_idx+i]\n",
    "                tagged = tokenizer.convert_tokens_to_string(tagged)\n",
    "                print(tagged)\n",
    "                break        \n",
    "    else:\n",
    "        print(f'{tagtype} does not exist.')\n",
    "        pass\n",
    "\n",
    "dat = 'DAT'\n",
    "tim = 'TIM'\n",
    "loc = 'LOC'\n",
    "wrk = 'WRK'\n",
    "\n",
    "tagtype = loc\n",
    "\n",
    "extract(tagtype, tags)\n",
    "\n",
    "### source code\n",
    "# \n",
    "# tagtype = dat\n",
    "# b_idx = tags.index(f'{tagtype}_B')\n",
    "# for i, tag in enumerate(tags[b_idx:]):\n",
    "#     if i > 0 and tag != f'{tagtype}_I':\n",
    "#         print(i)\n",
    "#         print(tag)\n",
    "#         print(tags[b_idx:b_idx+i])\n",
    "#         print(subwords[b_idx:b_idx+i])\n",
    "#         print(tokenizer.convert_tokens_to_string(subwords[b_idx:b_idx+i]))\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair Inputs and Split to Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pair = []\n",
    "num = len(input_tokens)\n",
    "for i in range(num):\n",
    "    tmp_token_idx = input_tokens[i]\n",
    "    tmp_mask = input_mask[i]\n",
    "    tmp_label_idx = input_labels[i]\n",
    "    input_pair.append((tmp_token_idx, tmp_mask, tmp_label_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = random.randrange(len(input_tokens))\n",
    "# print(input_pair[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋을 8:2 비율로 훈련 데이터셋과 검증 데이터셋으로 분할합니다. 데이터셋의 양이 적어 테스트셋은 생성하지 않았습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train_test_split(input_pair, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3637\n",
      "910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(train)), print(len(valid))\n",
    "# print(train[0]), print(valid[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = []\n",
    "# for e in train[0][0]:\n",
    "#     encoded = tokenizer.convert_tokens_to_ids(str(e))\n",
    "#     temp.append(encoded)\n",
    "# print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delcare Dataset (Iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch에서 데이터를 받아들일 수 있도록 커스텀 데이터셋을 생성합니다. 이 때 문장을 구성하는 토큰들을 고유한 인덱스 번호로 변환합니다. 마찬가지로 레이블도 인덱스 번호로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset): \n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "    def __len__(self): \n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.data[idx][0]\n",
    "        input_masks_lst = self.data[idx][1]\n",
    "        labels = self.data[idx][2]\n",
    "        input_ids_lst = []\n",
    "        for token in tokens:\n",
    "            ids = tokenizer.convert_tokens_to_ids(str(token))\n",
    "            input_ids_lst.append(ids)\n",
    "        label_ids = []\n",
    "        for label in labels:\n",
    "            label_ids.append(label_dict[label])\n",
    "                \n",
    "        return (torch.LongTensor(input_ids_lst), torch.LongTensor(input_masks_lst), torch.LongTensor(label_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CustomDataset(train, tokenizer)\n",
    "validset = CustomDataset(valid, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 크기가 128인 데이터로더를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size = 128, shuffle = True)\n",
    "valid_loader = DataLoader(validset, batch_size = 128, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "256\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "for i, el in enumerate(train_loader):\n",
    "    print(len(el[0][0]))\n",
    "    print(len(el[1][0]))\n",
    "    print(len(el[2][0]))\n",
    "    # print(el[0][0])\n",
    "    # print(el[1][0])\n",
    "    # print(el[2][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('jeonghyeon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df12b971f0e4e081474c4ac44bd338416eac6f5401e1e938ba342788cee78ecd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
