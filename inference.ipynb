{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module.model import model as tagger\n",
    "from module.model import device\n",
    "from module.model import tokenizer\n",
    "from module.model import id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-q KEYWORD]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9003 --control=9001 --hb=9000 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"9b745e73-fdfa-47a4-a3d7-2e5b936047b6\" --shell=9002 --transport=\"tcp\" --iopub=9004 --f=/home/ubuntu/.local/share/jupyter/runtime/kernel-v2-17239VsodKbOR2SWS.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/jeonghyeon/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3406: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-q\", \"--query\", dest=\"keyword\", action=\"store\")\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = os.listdir('./inference_data')\n",
    "file_path = []\n",
    "for file_name in file_list:\n",
    "    tmp_path = os.path.join('./inference_data', file_name)\n",
    "    file_path.append(tmp_path)\n",
    "file_path.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of ./inference_data/event_port.csv: 171\n"
     ]
    }
   ],
   "source": [
    "load_df = lambda x: pd.read_csv(x)\n",
    "dfs = [load_df(x) for x in file_path]\n",
    "data = dfs[0]\n",
    "data.dropna(inplace=True)\n",
    "# data[pd.isna(data).any(axis=1)]\n",
    "# data.isna().nunique()\n",
    "print(f'length of {file_path[0]}: {len(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Event    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(df):\n",
    "#     df['Event'] = df['Event'].str.replace(\"．\", \".\", regex=False)\n",
    "#     df['Event'] = df['Event'].astype(str)\n",
    "#     # df['Event'] = df['Event'].str.replace(r'[^ㄱ-ㅣ가-힣0-9a-zA-Z.]+', \" \", regex=True)\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = '화물'\n",
    "\n",
    "data['Indexes'] = data.Event.str.find(keyword)\n",
    "infer_data = data[data.Indexes > -1]\n",
    "# infer_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for event in infer_data.Event:\n",
    "#     print(event)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 36)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_data_lst = infer_data.Event.apply(lambda x: tokenizer(x, max_length=256, padding='max_length', truncation=True))\n",
    "input_ids_lst = []\n",
    "input_masks_lst = []\n",
    "for e in infer_data_lst:\n",
    "    input_ids_lst.append(e['input_ids'])\n",
    "    input_masks_lst.append(e['attention_mask'])\n",
    "len(input_ids_lst), len(input_masks_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(input_ids_lst, input_masks_lst):\n",
    "\n",
    "    tagger.eval()\n",
    "    with torch.no_grad():\n",
    "        infer_results = []\n",
    "        for input_ids, input_masks in zip(input_ids_lst, input_masks_lst): \n",
    "            input_tensor = torch.LongTensor(input_ids).unsqueeze(0).to(device)\n",
    "            mask_tensor = torch.LongTensor(input_masks).unsqueeze(0).to(device)\n",
    "            output = tagger(input_tensor, mask_tensor)\n",
    "            pred = torch.argmax(output, dim=-1).squeeze().detach().cpu().tolist()\n",
    "            infer_results.append(pred)\n",
    "\n",
    "        return infer_results\n",
    "    \n",
    "infer_results = infer(input_ids_lst, input_masks_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "subwordsList = []\n",
    "tagsList = []\n",
    "toLabel = lambda x: id2label[x]\n",
    "for input_ids, result in zip(input_ids_lst, infer_results):\n",
    "    decoded = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "    subwords = tokenizer.encode(decoded)[1:-1]\n",
    "    subwords = list(map(lambda x: tokenizer.convert_ids_to_tokens(x), subwords))\n",
    "    subwordsList.append(subwords)\n",
    "    lenSubwords = len(subwords)    \n",
    "\n",
    "    result = result[1:lenSubwords+1]\n",
    "    tags = []\n",
    "    for id in result:\n",
    "        tags.append(toLabel(id))\n",
    "    tagsList.append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜(DAT), 시간(TIME), 장소(LOC), 작업 정보(WRK)를 추출해서 txt 파일에 저장\n",
    "\n",
    "def extract(tagtype, tags, subwords):\n",
    "    # if tag does not exists then notify and pass\n",
    "    # if not '{tag}_I' then stop\n",
    "    tag_start = f'{tagtype}_B'\n",
    "    if tag_start in tags:\n",
    "        b_idx = tags.index(tag_start)\n",
    "        for i, tag in enumerate(tags[b_idx:]):\n",
    "            if i > 0 and tag != f'{tagtype}_I':\n",
    "                # print(tags[b_idx:b_idx+i+1])\n",
    "                # print(tags[b_idx:b_idx+i])\n",
    "                tagged = subwords[b_idx:b_idx+i]\n",
    "                tagged = tokenizer.convert_tokens_to_string(tagged)\n",
    "                # print(tagged)\n",
    "                return tagged\n",
    "    else:\n",
    "        msg = f'{tagtype} not found.'\n",
    "        # print(msg)\n",
    "        return msg\n",
    "\n",
    "tagtypes = dict(\n",
    "    dat='DAT',\n",
    "    tim='TIM',\n",
    "    loc='LOC',\n",
    "    wrk='WRK',    \n",
    ")\n",
    "\n",
    "# subwordsList, tagsList\n",
    "nerResults = []\n",
    "count = 0\n",
    "for subwords, tags in zip(subwordsList, tagsList):\n",
    "    # print(f'index: {count}')\n",
    "    extracted = []\n",
    "    for tagtype in tagtypes.values():\n",
    "        # print(tagtype)\n",
    "        tagged = extract(tagtype, tags, subwords)\n",
    "        extracted.append(tagged)\n",
    "        # print()\n",
    "    nerResults.append(extracted)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in nerResults:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(nerResults, columns=['Date', 'Time', 'Location', 'Work'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'], df['Time'], df['Location'], df['Work'] = df['Date']+'\\t', '\\t'+df['Time']+'\\t', '\\t'+df['Location']+'\\t', '\\t'+df['Work']\n",
    "df.to_csv('./inference_result.txt', index=False, sep='|')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('jeonghyeon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df12b971f0e4e081474c4ac44bd338416eac6f5401e1e938ba342788cee78ecd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
